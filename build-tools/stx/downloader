#!/usr/bin/python3

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Copyright (C) 2021-2022 Wind River Systems,Inc

# import apt
import apt_pkg
import argparse
import debrepack
import discovery
import fnmatch
import glob
import logging
import os
import pathlib
import repo_manage
import shutil
import signal
import subprocess
import sys
import utils

# make ourself nice
pid = os.getpid()
os.setpriority(os.PRIO_PROCESS, 0, 15)
subprocess.run(['ionice', '-c', '3', '-p', str(pid)])

DEFAULT_ARCH = 'amd64'
REPO_BIN = 'deb-local-binary'
mirror_root = os.environ.get('OS_MIRROR')
stx_src_mirror = os.path.join(mirror_root, 'sources')
stx_bin_mirror = os.path.join(mirror_root, 'binaries')

STX_DEFAULT_DISTRO = discovery.STX_DEFAULT_DISTRO
STX_DEFAULT_DISTRO_CODENAME = discovery.STX_DEFAULT_DISTRO_CODENAME
DIST_CODENAME = os.environ.get('DIST', STX_DEFAULT_DISTRO_CODENAME)
all_binary_lists = ['base-' + DIST_CODENAME + '.lst', 'os-std.lst', 'os-rt.lst']

logger = logging.getLogger('downloader')
utils.set_logger(logger)

rlogger = logging.getLogger('repo_manager')
utils.set_logger(rlogger)

ALL_DISTROS = discovery.get_all_distros()
ALL_LAYERS = discovery.get_all_layers(distro=STX_DEFAULT_DISTRO, codename=STX_DEFAULT_DISTRO_CODENAME)
ALL_BUILD_TYPES = discovery.get_all_build_types(distro=STX_DEFAULT_DISTRO)

STX_MIRROR_STRATEGY = os.environ.get('STX_MIRROR_STRATEGY')
if STX_MIRROR_STRATEGY is None:
    STX_MIRROR_STRATEGY = "stx_mirror_first"

apt_rootdir = '/usr/local/apt-chroot'

apt_conf_content = f"""\
Dir "/";
Dir::State "/var/lib/apt";
Dir::State::Lists "/var/lib/apt/lists";
Dir::Cache "/var/cache/apt";
Dir::Cache::Archives "/var/cache/apt/archives";
Dir::Etc "/etc/apt";
Dir::Etc::Trusted "/etc/apt/trusted.gpg";
Dir::Etc::TrustedParts "/etc/apt/trusted.gpg.d";
Acquire::Check-Valid-Until "false";
"""

apt_conf_chroot_content = f"""\
Dir "{apt_rootdir}";
Dir::State "{apt_rootdir}/var/lib/apt";
Dir::State::Lists "{apt_rootdir}/var/lib/apt/lists";
Dir::Cache "{apt_rootdir}/var/cache/apt";
Dir::Cache::Archives "{apt_rootdir}/var/cache/apt/archives";
Dir::Etc "{apt_rootdir}/etc/apt";
Dir::Etc::Trusted "{apt_rootdir}/etc/apt/trusted.gpg";
Dir::Etc::TrustedParts "{apt_rootdir}/etc/apt/trusted.gpg.d";
Acquire::Check-Valid-Until "false";
"""

apt_required_paths = [
    "/etc/apt",
    "/var/lib/apt",
]


def get_version(pkg, version_str):
    """
    Given an apt_pkg.Package object, return a apt_pkg.version objectmatching version_str
    """
    for ver in pkg.version_list:
        if version_str is None:
            return ver
        if ver.ver_str == version_str:
            return ver
    return None

def get_avail_versions(pkg):
    """
    Given an apt_pkg.Package object, return a list of version strings
    """
    vers = []
    for ver in pkg.version_list:
        vers.append(ver.ver_str)
    return vers

def parse_packages_file(file_path, target_package, target_version):
    with open(file_path, 'r') as f:
        entry = {}
        for line in f:
            line = line.strip()
            if not line:
                # End of entry
                if entry.get("Package") == target_package and entry.get("Version") == target_version:
                    return entry
                entry = {}
            elif ": " in line:
                key, value = line.split(": ", 1)
                entry[key] = value
            elif line.startswith(" ") and entry:
                # Multi-line value continuation
                last_key = list(entry.keys())[-1]
                entry[last_key] += "\n" + line.strip()
    return None

def get_download_urls(pkg_name, target_version, version):
    """
    Given an apt_pkg.Version object, return a list of full .deb download URLs.
    """
    urls = []
    lists_dir = os.path.join(apt_rootdir, "var/lib/apt/lists")
    #
    for pkg_file_tuple in version.file_list:
        pkg_file_obj, _ = pkg_file_tuple
        # site = pkg_file_obj.site  # e.g. 'stx-master-deb-stx-repomgr:80'
        # label = pkg_file_obj.label    # e.g. 'deb-local-binary trixie'
        # archive = pkg_file_obj.archive  # e.g. 'trixie'
        # component = pkg_file_obj.component  # e.g. 'main'
        # arch = pkg_file_obj.architecture  # e.g. 'main'
        index_path = pkg_file_obj.filename  # e.g. '/tmp/downloader/var/lib/apt/lists/stx-master-deb-stx-repomgr:80_deb-local-binary_dists_trixie_main_binary-amd64_Packages'
        #
        # Construct the base URL (example, adjust if your repo structure differs)
        base=os.path.relpath(index_path, lists_dir).replace('_','/').split("/dists/", 1)[0]
        base_url = f"http://{base}"
        #
        if os.path.exists(index_path):
            entry = parse_packages_file(index_path, pkg_name, target_version)
            if entry and "Filename" in entry:
                relpath = entry["Filename"].lstrip("/")
                final_url = f"{base_url}/{relpath}"
                urls.append(final_url)
    #
    return urls


def get_downloaded(dl_dir, dl_type):
    """
    Browse and get the already downloaded binary or source
    packages in dl_dir
        dl_dir: mirror dir
        dl_type: binary or source
        return: list of downloaded targets
    """
    dl_list = []
    if not os.path.exists(dl_dir):
        return []

    if dl_type == 'source':
        logger.debug('debrepack will check the whole source mirror')

    for file in os.listdir(dl_dir):
        if dl_type == 'binary' and file.endswith('.deb'):
            dl_list.append(file)

    return dl_list


def get_pkgs_from_list(root_dir, list_file):
    """
    Read each lines in debian_pkg_dirs_<type> and add it
    to the map, for example:
    entries { 'dhcp': '<path to>/stx/integ/base/dhcp',
              'tsconfig': '<path to>/config/tsconfig'}
    """
    entries = {}
    try:
        with open(list_file, 'r') as flist:
            lines = list(line for line in (p.strip() for p in flist) if line)
    except Exception as e:
        logger.error(str(e))
    else:
        for entry in lines:
            entry = entry.strip()
            if entry.startswith('#'):
                continue
            entries[os.path.basename(entry)] = os.path.join(root_dir, entry)
    return entries


def get_all_stx_pkgs():
    """
    Scan all STX source layers to get all buildable packages
    Params: None
    Return:
        Map of all STX buildable packages and path to debian folder
    """
    pkgs = {}
    for pkgs_file in discovery.get_pkg_dirs_files(layer='all'):
        root = os.path.basename(pkgs_file)
        pkgs.update(get_pkgs_from_list(root, pkgs_file))
    return pkgs


def get_all_binary_list(distro=STX_DEFAULT_DISTRO, codename=STX_DEFAULT_DISTRO_CODENAME, layers=None,
                        build_types=None):
    """
    Return all binary packages listed in base-${DIST_CODENAME}.lst,
    os-std.lst,os-rt.lst
    """
    layer_binaries = {}
    stx_config = os.path.join(os.environ.get('MY_REPO_ROOT_DIR'),
                              'stx-tools/debian-mirror-tools/config/{}/{}'.format(
                                  distro, codename))
    if layers:
        for layer in layers:
            if layer not in ALL_LAYERS:
                logger.error(' '.join([layer, 'is not a valid layer']))
                return
    else:
        layers = ALL_LAYERS

    for layer in layers:
        if layer not in layer_binaries:
            layer_binaries[layer] = []
        search_dir = os.path.join(stx_config, layer)
        all_build_types = discovery.get_layer_build_types(layer, distro=distro, codename=codename)
        if not all_build_types:
            logger.error('No build_types found for {}/{}, layer: {}'.format(distro, codename, layer))
            return

        if not build_types:
            build_types = all_build_types

        for build_type in build_types:
            if build_type not in all_build_types:
                logger.warning('{} is not a valid build_type for {}/{} of layer: {}'.format(
                    build_type, distro, codename, layer))
                continue

            pattern=''.join(['os-',build_type,'.lst'])
            for root, dirs, files in os.walk(search_dir):
                for f in fnmatch.filter(files, pattern):
                    layer_binaries[layer].append(os.path.join(root, f))
        logger.info(
            f"Binary lists for layer `{layer}`: "
            f"{layer_binaries[layer]}"
        )

    search_dir = os.path.join(stx_config, 'common')
    pattern='base-' + codename + '*.lst'

    if "common" not in layer_binaries:
        layer_binaries["common"] = []

    for root, dirs, files in os.walk(search_dir):
        for f in fnmatch.filter(files, pattern):
            layer_binaries["common"].append(os.path.join(root, f))

    logger.info(
        f"Binary lists for layer `common`: "
        f"{layer_binaries['common']}"
    )
    return layer_binaries

            
def run_in_chroot(cmd):
    """Run a shell command inside the chroot"""
    full_cmd = ["sudo", "chroot", apt_rootdir] + cmd
    print(f"Running inside chroot: {' '.join(cmd)}")
    subprocess.run(full_cmd, check=True)

def run_in_chroot_capture(cmd):
    """Run command in chroot and capture output"""
    full_cmd = ["sudo", "chroot", apt_rootdir] + cmd
    result = subprocess.run(full_cmd, check=True, capture_output=True, text=True)
    return result.stdout.strip()

def kill_gpg_agent_in_chroot():
    try:
        run_in_chroot(["pkill", "-u", "root", "gpg-agent"])
        run_in_chroot(["pkill", "-u", "root", "dirmngr"])
        print("gpg-agent inside chroot killed.")
    except subprocess.CalledProcessError:
        print("No gpg-agent process found inside chroot (or already exited).")

def extract_and_split_keys_in_chroot(combined_keyring, output_dir):
    # Temporary keyring inside chroot
    temp_keyring = "/tmp/temp.gpg"

    # Import combined keyring into temp keyring
    run_in_chroot([
        "gpg", "--no-default-keyring",
        "--keyring", temp_keyring,
        "--import", combined_keyring
    ])

    # List keys from temp keyring
    output = run_in_chroot_capture([
        "gpg", "--no-default-keyring",
        "--keyring", temp_keyring,
        "--list-keys", "--with-colons"
    ])

    # Extract key fingerprints from 'pub' lines
    key_ids = [line.split(":")[4] for line in output.splitlines() if line.startswith("pub")]

    for keyid in key_ids:
        keyfile = os.path.join(output_dir, f"{keyid}.gpg")
        if os.path.isfile(f"{apt_rootdir}{keyfile}"):
            continue
        print(f"Exporting key {keyid} to {keyfile} inside chroot...")
        # Export key and write to file inside chroot
        with open(f"{apt_rootdir}{keyfile}", "wb") as f:
            export_proc = subprocess.run(
                ["sudo", "chroot", apt_rootdir,
                 "gpg", "--no-default-keyring",
                 "--keyring", temp_keyring,
                 "--export", keyid],
                check=True, stdout=subprocess.PIPE
            )
            f.write(export_proc.stdout)
        os.chmod(f"{apt_rootdir}{keyfile}", 0o644)

    # Clean up temp keyring file inside chroot
    run_in_chroot(["rm", "-f", temp_keyring])
    kill_gpg_agent_in_chroot()
    print("Key export inside chroot complete.")

def combine_keys_in_chroot():
    # Temporary keyring inside chroot
    temp_keyring = "/tmp/temp.gpg"
    combined_keyring = "/etc/apt/trusted.gpg"
    partial_keyring_dir = "/etc/apt/trusted.gpg.d"

    # Clean up temp keyring file inside chroot
    run_in_chroot(["rm", "-f", temp_keyring])

    # Import combined keyring into temp keyring
    run_in_chroot([
        "bash", "-c",
        "for f in /etc/apt/trusted.gpg.d/*.gpg; do " +
        "gpg --no-default-keyring --keyring $f --export >> " + temp_keyring + "; " +
        "done"
    ])

    # Copy temp keyring into legacy location for apt unified key
    run_in_chroot(["mv", "-f", temp_keyring, combined_keyring])

    # Set ownership of combined keyring
    run_in_chroot(["chown", "root:root", "-R", combined_keyring, partial_keyring_dir])

    # make sure there is no gpg_agent process left running in chroot that might prevent umount
    kill_gpg_agent_in_chroot()
    print("Key preperation inside chroot complete.")

def create_apt_chroot():
    # Step 0: clean previous chroot
    if os.path.exists(apt_rootdir):
        for path in [ "/proc", ]:
            mount_path = os.path.join(apt_rootdir, path.lstrip("/"))
            if not os.path.ismount(mount_path):
               subprocess.run([
                  "sudo", "mount", "-o", "bind", path, os.path.join(apt_rootdir, path.lstrip("/"))
               ], check=True)
        kill_gpg_agent_in_chroot()
        for path in [
            "/proc",
            "/sys",
            "/dev/pts",
            "/dev",
            "/etc/resolv.conf"
            ]:
            mount_path = os.path.join(apt_rootdir, path.lstrip("/"))
            if os.path.ismount(mount_path):
                subprocess.run([
                    "sudo", "umount", mount_path
                ], check=True)
        for path in [
            "var/lib/apt/lists/partial",
            "var/cache/apt/archives/partial"
            ]:
            path = os.path.join(apt_rootdir, path.lstrip("/"))
            if os.path.exists(path):
                subprocess.run([
                    "sudo", "chown", str(os.getuid()), path
                ], check=True)



    # if os.path.isdir(apt_rootdir):
    #     subprocess.run([
    #         "sudo", "rm", "-rf", apt_rootdir
    #     ])

    # Step 1: If missing apt_rootdir (running on an old container) then run debootstrap
    if not os.path.exists(apt_rootdir):
        os.makedirs(apt_rootdir)
        subprocess.run([
            "fakeroot", "debootstrap", "--variant=minbase", "--include=ca-certificates,debian-archive-keyring,gnupg,procps", "--foreign", DIST_CODENAME, apt_rootdir, "http://deb.debian.org/debian"
        ], check=True)

    # Step 2: Copy current system's APT sources into chroot
    etc_apt = "/etc/apt"
    key_src_dir = "/usr/share/keyrings"
    key_dst_dir = "/etc/apt/trusted.gpg.d"
    chroot_etc_apt = os.path.join(apt_rootdir, etc_apt.lstrip('/'))

    for path in [
        "var/lib/apt/lists/partial",
        "var/cache/apt/archives/partial",
        "etc/apt"
        ]:
        # os.makedirs(os.path.join(apt_rootdir, path.lstrip("/")), exist_ok=True)
        subprocess.run([
            "sudo", "mkdir", "-p", os.path.join(apt_rootdir, path.lstrip("/"))
        ], check=True)

    for path in [
        etc_apt,
        key_dst_dir
        ]:
        subprocess.run([
            "sudo", "chown", str(os.getuid()), os.path.join(apt_rootdir, path.lstrip("/"))
        ], check=True)

    # Copy /etc/apt/sources.list
    if os.path.exists("/etc/apt/sources.list"):
        shutil.copy("/etc/apt/sources.list", chroot_etc_apt)

    # Copy /etc/apt/sources.list.d/
    src_list_d = "/etc/apt/sources.list.d"
    dest_list_d = os.path.join(chroot_etc_apt, "sources.list.d")
    if os.path.exists(src_list_d):
        shutil.copytree(src_list_d, dest_list_d, dirs_exist_ok=True)

    # Create apt.conf for the non-standard location
    apt_config = os.path.join(chroot_etc_apt, 'apt.conf')
    apt_config_chroot = os.path.join(chroot_etc_apt, 'apt_chroot.conf')
    apt_config_d = os.path.join(etc_apt, 'apt.conf.d')
    apt_source_d = os.path.join(etc_apt, 'sources.list.d')

    with open(apt_config, "w") as f:
        f.write(apt_conf_content)
    with open(apt_config_chroot, "w") as f:
        f.write(apt_conf_chroot_content)

    for path in [
        "/proc",
        "/sys",
        "/dev",
        "/etc/resolv.conf"
        ]:
        subprocess.run([
           "sudo", "mount", "-o", "bind", path, os.path.join(apt_rootdir, path.lstrip("/"))
        ], check=True)

    subprocess.run([
        "sudo", "mount", "-t", "devpts", "devpts", os.path.join(apt_rootdir, "dev/pts")
    ], check=True)

    run_in_chroot(["rm", "-rf", apt_config_d])
    run_in_chroot(["rm", "-rf", apt_source_d])

    missing_keys = [ "debian-archive-keyring.gpg", "debian-archive-removed-keys.gpg" ]
    for missing_key in missing_keys:
        key_src = os.path.join(apt_rootdir, key_src_dir.lstrip('/'), missing_key)
        key_dst = os.path.join(apt_rootdir, key_dst_dir.lstrip('/'), missing_key)
        if not os.path.isfile(key_dst) and os.path.isfile(key_src):
            shutil.copy2(key_src, key_dst)

        extract_and_split_keys_in_chroot(os.path.join('/', key_dst_dir, missing_key),
                                         os.path.join('/', key_dst_dir))
    combine_keys_in_chroot()

    os.environ['APT_CONFIG'] = apt_config
    os.environ["DIR"] = apt_rootdir
    os.environ["DIR::State"] = os.path.join(apt_rootdir, "var/lib/apt")
    os.environ["DIR::Cache"] = os.path.join(apt_rootdir, "var/cache/apt")
    os.environ["DIR::Etc"] = os.path.join(apt_rootdir, "etc/apt")

    print("Chroot created and APT sources synced.")

def apt_update_inside_chroot():
    # run_in_chroot([ "apt-get", "update" ])
    subprocess.run([
        "sudo", "chroot", apt_rootdir, "apt-get", "update"
    ])


class BaseDownloader():
    def __init__(self, arch, _dl_dir, dl_list_file, clean):
        self.dl_dir = _dl_dir
        self.arch = arch
        self.clean_mirror = clean
        self.dl_need = []
        self.dl_success = []
        self.dl_failed = []
        self.repomgr = repo_manage.RepoMgr('aptly', os.environ.get('REPOMGR_URL'),
                                           '/tmp/', os.environ.get('REPOMGR_ORIGIN'),
                                           rlogger)
        self.dl_list_fh = open(dl_list_file, 'w')

    def clean(self):
        if os.path.exists(self.dl_dir):
            if self.clean_mirror:
                try:
                    shutil.rmtree(self.dl_dir)
                except Exception as e:
                    logger.error(str(e))
                    logger.critical("Failed to clean mirror %s", self.dl_dir)
                    sys.exit(1)
                else:
                    logger.debug("Successfully cleaned mirror %s", self.dl_dir)
        os.makedirs(self.dl_dir, exist_ok=True)

    def reports(self):
        ret = 0
        if len(self.dl_need):
            logger.info("++++++++++++++++++++++++++++++++++++++++++++++++++")
            logger.info("Total number of packages needing to be downloaded: %d", len(self.dl_need))

        if len(self.dl_success):
            logger.info("++++++++++++++++++++++++++++++++++++++++++++++++++")
            logger.info("Successfully downloaded packages: %d", len(self.dl_success))
            for dlobj in sorted(self.dl_success):
                logger.info(' '.join(['-', dlobj.strip()]))

        failed_list = list(set(self.dl_need) - set(self.dl_success))
        if len(failed_list):
            logger.error("+++++++++++++++++++++++++++++++++++++++++++++++++")
            logger.error("Failed to download packages: %d", len(failed_list))
            ret = 1
            for dlobj in sorted(failed_list):
                logger.error(' '.join([dlobj.strip()]))
        return ret

    def list_failed_pkgs(self):
        failed_list = list(set(self.dl_need) - set(self.dl_success))
        if len(failed_list):
            logger.error("Packages failed to download:")
            for dlobj in sorted(failed_list):
                logger.error(' '.join([dlobj.strip()]))


    def save_dl_file_names(self, filename_list):
        for filename in filename_list:
            print (filename, file=self.dl_list_fh)
        self.dl_list_fh.flush()


class DebDownloader(BaseDownloader):
    def __init__(self, arch, _dl_dir, dl_list_file, force, _layer_binaries):
        super(DebDownloader, self).__init__(arch, _dl_dir, dl_list_file, force)
        self.need_download = []
        self.downloaded = []
        self.need_upload = []
        self.layer_binaries = _layer_binaries
        apt_pkg.config.set("Dir::Root", apt_rootdir)
        apt_pkg.config.set("Dir::Etc::main", apt_rootdir + '/etc/apt/apt_chroot.conf')
        apt_pkg.init()
        self.apt_cache = apt_pkg.Cache()

    def _get_layer_binaries_repository(self, layer: str):
        repo = REPO_BIN
        if layer != "common":
            repo = f"{REPO_BIN}-{layer.lower()}"
        return repo

    def create_binary_repo(self):
        if not self.repomgr:
            logger.error("The repo manager is not created")
            return False

        for layer in self.layer_binaries:
            repo = self._get_layer_binaries_repository(layer)
            try:
                self.repomgr.upload_pkg(repo, None)
            except Exception as e:
                logger.error(str(e))
                logger.error("Failed to create repository %s", repo)
                return False

            logger.info("Successfully created repository %s", repo)
        return True

    
    def get_arch_of_version(self, ver):
        """
        """
        apt_records = apt_pkg.PackageRecords(self.apt_cache)
        apt_records.lookup(ver.file_list[0])
        record_str = apt_records.record
        fields = dict(
            line.split(": ", 1) for line in record_str.strip().split("\n") if ": " in line
        )
        arch = fields['Architecture']
        return arch

    def download(self, _name, _version, dl_file, url=None, retries=3):
        logger.info ('download _name=%s _version=%s dl_file=%s url=%s retries=%s', _name, _version, dl_file, str(url), retries)
        if url is not None:
            ret = os.path.join(self.dl_dir, dl_file)
            tmp_file = ".".join([ret, "tmp"])
            utils.run_shell_cmd(["rm", "-rf", tmp_file], logger)
            (dl_url, alt_dl_url) = utils.get_download_url(url, STX_MIRROR_STRATEGY)
            for i in range(1,retries+1):
                if alt_dl_url:
                    try:
                        utils.run_shell_cmd(["curl", "-k", "-L", "-f", dl_url, "-o", tmp_file], logger)
                    except:
                        if i < retries:
                            try:
                                utils.run_shell_cmd(["curl", "-k", "-L", "-f", alt_dl_url, "-o", tmp_file], logger)
                                break
                            except Exception as e:
                                logger.error(str(e))
                        else:
                            utils.run_shell_cmd(["curl", "-k", "-L", "-f", alt_dl_url, "-o", tmp_file], logger)
                            break
                else:
                    if i < retries:
                        try:
                            utils.run_shell_cmd(["curl", "-k", "-L", "-f", dl_url, "-o", tmp_file], logger)
                            break
                        except Exception as e:
                            logger.error(str(e))
                    else:
                        utils.run_shell_cmd(["curl", "-k", "-L", "-f", dl_url, "-o", tmp_file], logger)
                        break
            utils.run_shell_cmd(["mv", tmp_file, ret], logger)
            return ret

        try:
            package = self.apt_cache[_name]
            if not package:
                logger.error(' '.join(['Failed to find', _name,
                             'in available sources']))
                logger.error('You may need to update the package list file')

            candidate = get_version(package, _version)
            if not candidate:
                logger.error(' '.join(['Failed to download', _name,
                             'with wrong version', _version, '?']))
                logger.error('You may need to update the package list file')
                logger.error('package: %s', str(package))
                logger.error('Available versions: %s', str(get_avail_versions(package)))
                return None

            urls = get_download_urls(_name, _version, candidate)
            logger.info ('Downloading %s from %s', dl_file, str(urls))
            # ret = candidate.fetch_binary(self.dl_dir)
            for url in urls:
                ret = self.download(_name, _version, dl_file, url=url, retries=retries)
                if ret:
                    break
            assert os.path.basename(ret) == dl_file
            if ret:
                return ret
        except Exception as e:
            deb_name = _name + '_' + _version
            logger.debug("Failed to fetch binary %s", deb_name)
            logger.debug(str(e))
            '''
            Sometimes the target deb is created in dl_dir, but actually it is
            corrupted file. It should not be uploaded to binary repo.
            '''
            os.system('rm -f ' + os.path.join(self.dl_dir, deb_name + '*.deb'))
        return None


    def reports(self):
        for layer in self.layer_binaries:
            repo = self._get_layer_binaries_repository(layer)
            try:
                self.repomgr.deploy_repo(repo)
            except Exception as e:
                logger.error(str(e))
                logger.error("Failed to publish repository %s", repo)
                return

            if self.layer_binaries[layer]:
                logger.info(f"[{layer}] Binary list:")
                for bin_list in self.layer_binaries[layer]:
                    logger.info(bin_list)

        logger.info("Show result for binary download:")
        return super(DebDownloader, self).reports()

    def download_list_files(self, repo, list_files):
        pkg_data=[]
        if len(list_files):
            for list_file in list_files:
                if not os.path.exists(list_file):
                    continue
                with open(list_file) as flist:
                    lines = list(line for line in (lpkg.strip() for lpkg in flist) if line)
                    for pkg in lines:
                        pkg = pkg.strip()
                        if pkg.startswith('#'):
                            continue
                        pkg_name_array = pkg.split()
                        pkg_name = pkg_name_array[0]
                        if len(pkg_name_array) == 1:
                            logger.error("The package version of %s should be defined in file %s", pkg_name, list_file)
                            logger.error("Please update the list file %s", list_file)
                            sys.exit(1)

                        # strip epoch
                        ver_array = pkg_name_array[1].split(":")
                        if len(ver_array) == 1:
                            pkg_ver = ver_array[0]
                            pkg_epoch = None
                        else:
                            pkg_ver = ver_array[-1]
                            pkg_epoch = ver_array[0]

                        if len(pkg_name_array) == 3:
                            url = pkg_name_array[2]
                            url_dict = utils.deb_file_name_to_dict(os.path.basename(url).replace("%2B", "+"))
                            logger.debug("dkg_data: name=%s, ver=%s, url=%s, url_dict=%s, file=%s", pkg_name, pkg_ver, url, str(url_dict), list_file)
                            if url_dict['ver'] and url_dict['ver'] != pkg_ver:
                                logger.warning("Package version mismatch for package %s, %s vs %s, in file %s", pkg_name, pkg_ver, url_dict['ver'], list_file)
                                pkg_ver = url_dict['ver']
                            if url_dict['epoch'] and url_dict['epoch'] != pkg_epoch:
                                logger.warning("Package epoch mismatch for package %s, $s vs %s, in file %s", pkg_name, pkg_epoch, url_dict['epoch'], list_file)
                                pkg_epoch = url_dict['epoch']

                            # Get arch from filename
                            arch = pathlib.Path(url).stem.split("_")[-1]
                        else:
                            url = None
                            try:
                                package = self.apt_cache[pkg_name]
                                candidate = get_version(package, pkg_ver)
                                if candidate is None:
                                    candidate = get_version(package, None)
                                arch = self.get_arch_of_version(candidate)
                            except Exception as e:
                                logger.error(str(e))
                                sys.exit(1)

                        pkg_dict={'name':pkg_name, 'ver':pkg_ver, 'epoch':pkg_epoch, 'arch':arch, 'url':url, 'repo':repo}
                        pkg_data.append(pkg_dict)

        self.download_list(repo, pkg_data)


    def download_list(self, repo, pkg_data):
        logger.info(' '.join(['pkg_data:', str(pkg_data)]))

        # List of packages already downloaded
        self.downloaded = get_downloaded(self.dl_dir, 'binary')
        logger.info(' '.join(['previously downloaded:', str(self.downloaded)]))

        # list of package already uploaded to ANY repo
        previously_uploaded = self.repomgr.list_pkgs(repo)
        logger.info(' '.join(['previously uploaded to repo', repo, ':', str(previously_uploaded)]))

        used_dl_files = []
        pkg_data_map = {}
        if pkg_data:
            for pkg_dict in pkg_data:
                pkg_name = pkg_dict['name']
                pkg_ver = pkg_dict['ver']
                pkg_epoch = pkg_dict['epoch']
                arch = pkg_dict['arch']
                url = pkg_dict['url']
                repo = pkg_dict['repo']
                if pkg_name not in pkg_data_map:
                    pkg_data_map[pkg_name] = []
                pkg_data_map[pkg_name].append(pkg_dict)
                pkg_name_ver = '_'.join([pkg_name, pkg_ver])
                if pkg_epoch:
                    pkg_name_epoch_ver = '_'.join([pkg_name, ':'.join([pkg_epoch, pkg_ver])])
                else:
                    pkg_name_epoch_ver = pkg_name_ver

                pname_arch = '_'.join([pkg_name_ver, arch]) + '.deb'
                pname_epoch_arch = '_'.join([pkg_name_epoch_ver, arch]) + '.deb'

                self.dl_need.append(pkg_name_ver)

                if self.downloaded and pname_arch in self.downloaded:
                    logger.debug(''.join([pname_epoch_arch, ' has been downloaded, skip']))
                    self.dl_success.append(pkg_name + '_' + pkg_ver)
                    self.need_upload.append([pname_arch, pname_epoch_arch])
                    self.save_dl_file_names([pname_arch])
                else:
                    # Tests show that the 'epoch' should be taken when
                    # fetch the package with 'apt' module, there is not 'epoch'
                    # in the dowloaded package name. This also requires the 'epoch'
                    # should be defined in the package list file with ':'
                    self.need_download.append([pname_arch, pkg_name_epoch_ver, url])

        # Download packages
        for debs in self.need_download:
            pname_arch = debs[0]
            pname_epoch_arch = debs[1]
            url = debs[2]
            logger.debug(' '.join(['package', pname_epoch_arch, 'needs to be downloaded']))
            debnames = pname_epoch_arch.split('_')
            deb_name = debnames[0]

            ret = self.download(debnames[0], debnames[1], pname_arch, url)
            if ret:
                self.save_dl_file_names([os.path.basename (ret)])
                deb_ver = debnames[1].split(":")[-1]
                deb_ver_epoch = '_'.join([debnames[0], debnames[1]])
                logger.info(' '.join([deb_ver_epoch, ' download ok']))
                # strip epoch
                self.dl_success.append('_'.join([debnames[0], deb_ver]))
                self.need_upload.append([pname_arch, pname_epoch_arch])
                if previously_uploaded and deb_ver_epoch in previously_uploaded:
                    try:
                        del_ret = self.repomgr.delete_pkg(repo, deb_name, 'binary', deb_ver)
                        logger.debug("deleted the old %s from repo %s, ret %d", deb_name, repo, del_ret)
                    except Exception as e:
                        logger.error(str(e), exc_info=True)
                        logger.error("Exception on deleting %s from %s", deb_name, repo)
            else:
                self.dl_failed.append(pname_epoch_arch)

        self.need_download.clear()

        logger.info(' '.join(['need_upload', str(self.need_upload)]))

        # Delete previously uploaded packages that are no longer needed
        for prev_upload in previously_uploaded:
            prev_upload_dict = utils.deb_file_name_to_dict(prev_upload)
            del_name = prev_upload_dict['name']
            delete_me = True 
            # Verify the package is no londer needed    
            if pkg_data_map and del_name in pkg_data_map:
                for needed_dict in pkg_data_map[del_name]:
                    if  prev_upload_dict['ver']   == needed_dict['ver'] and \
                        prev_upload_dict['epoch'] == needed_dict['epoch'] and \
                        prev_upload_dict['arch']  == needed_dict['arch']:
                        # We still need this one
                        delete_me = False
                        continue
            if delete_me:
                del_ver = prev_upload_dict['ver']
                logger.debug("Deleting pkg %s_%s from %s", del_name, del_ver, repo)
                try:
                    del_ret = self.repomgr.delete_pkg(repo, del_name, 'binary', del_ver)
                except Exception as e:
                    logger.error(str(e))
                    logger.error("Exception on deleting %s from %s", '_'.join([del_name, del_ver]), repo)

        # Upload needed packages
        for debs in self.need_upload:
            deb_ver = debs[0]
            deb_ver_epoch = debs[1]
            deb_path = os.path.join(stx_bin_mirror, deb_ver)
            #  Search the package with the "epoch" in aptly repo
            if previously_uploaded and deb_ver_epoch in previously_uploaded:
                logger.info("%s has already been uploaded to %s, skip", deb_path, repo)
                continue

            deb_needed_dict = utils.deb_file_name_to_dict(deb_ver)
            logger.debug("Uploading pkg %s", deb_path)
            try:
                upload_ret = self.repomgr.upload_pkg(repo, deb_path, deploy=False)
            except Exception as e:
                logger.error(str(e))
                logger.error("Exception on uploading %s to %s", deb_path, repo)
                sys.exit(1)
            else:
                if upload_ret:
                    logger.debug("%s is uploaded to %s", deb_path, repo)
                else:
                    logger.error("Failed to upload %s to %s", deb_path, repo)
                    break

        self.need_upload.clear()


    def start(self):
        """Here define:
        the complete set of binaries = base_${DIST_CODENAME}.lst
                                     + <layer>/os-std.lst
                                     + <layer>/os-rt.lst
        """
        super(DebDownloader, self).clean()

        empty = True
        for layer in self.layer_binaries:
            if self.layer_binaries[layer]:
                for bin_list in self.layer_binaries[layer]:
                    empty = False
        if empty:
            logger.error("There are no lists of binary packages found")
            sys.exit(1)

        for layer in self.layer_binaries:
            repo = self._get_layer_binaries_repository(layer)
            if self.layer_binaries[layer]:
                self.download_list_files(repo, self.layer_binaries[layer])


class SrcDownloader(BaseDownloader):
    def __init__(self, arch, _dl_dir, dl_list_file, force,
                 distro=STX_DEFAULT_DISTRO, codename=STX_DEFAULT_DISTRO_CODENAME):
        super(SrcDownloader, self).__init__(arch, _dl_dir, dl_list_file, force)
        self.parser = None
        self.distro = distro
        self.codename =codename

    def prepare(self):
        build_dir = os.path.join(os.environ.get('MY_BUILD_PKG_DIR'))
        os.makedirs(build_dir, exist_ok=True)
        recipes_dir = os.path.join(os.environ.get('MY_BUILD_PKG_DIR'), 'recipes')
        os.makedirs(recipes_dir, exist_ok=True)
        if not self.parser:
            try:
                self.parser = debrepack.Parser(build_dir, recipes_dir, log_level='debug',
                                               distro=self.distro, codename=self.codename)
            except Exception as e:
                logger.error(str(e))
                logger.error("Failed to create debrepack parser")
                return False

        return True

    def download_pkg_src(self, _pkg_path)->list[str]:
        if not self.parser:
            return None
        try:
            return self.parser.download(_pkg_path, self.dl_dir)
        except Exception as e:
            logger.error(str(e))
            logger.error("Failed to download source with %s", _pkg_path)
            return None

    def download_all(self, layers=None, build_types=None)->list[str]:
        logger.info("download_all, layers=%s, build_types=%s" % (layers, build_types))
        if layers:
            for layer in layers:
                if layer not in ALL_LAYERS:
                    logger.error(' '.join([layer, 'is not a valid layer']))
                    return
        else:
            layers = ALL_LAYERS

        pkg_dirs = []

        for layer in layers:
            all_build_types = discovery.get_layer_build_types(layer, distro=self.distro, codename=self.codename)
            if not all_build_types:
                logger.error('No build_types found for {}/{}, layer {}'.format(self.distro, self.codename, layer))
                return

            if not build_types:
                build_types = all_build_types

            for build_type in build_types:
                if build_type not in all_build_types:
                    logger.warning(' '.join([build_type, 'is not a valid build_type for distro', distro, 'of layer', layer]))
                    continue

                pkg_dirs.extend(discovery.package_dir_list(distro=self.distro, codename=self.codename, layer=layer,
                                                           build_type=build_type))

        if not len(pkg_dirs):
            logger.info("No source packages found")
            return

        pkg_dirs_to_names = discovery.package_dirs_to_names_dict(pkg_dirs, distro=self.distro, codename=self.codename)
        for pkg_dir in pkg_dirs_to_names:
            self.dl_need.append(pkg_dirs_to_names[pkg_dir])

        logger.info("Starting to download %d source packages", len(pkg_dirs))
        logger.info("%s", sorted(self.dl_need))
        for pkg_dir in pkg_dirs:
            dl_files = self.download_pkg_src(pkg_dir)
            if dl_files is not None:
                if pkg_dir in pkg_dirs_to_names:
                    self.dl_success.append(pkg_dirs_to_names[pkg_dir])
                self.save_dl_file_names (dl_files)
            else:
                if pkg_dir in pkg_dirs_to_names:
                    self.dl_failed.append(pkg_dirs_to_names[pkg_dir])

    def start(self, layers=None, build_types=None):
        # stx package source downloading
        super(SrcDownloader, self).clean()

        if self.prepare():
            self.download_all(layers=layers, build_types=build_types)
        else:
            logger.error("Failed to initialize source downloader")
            sys.exit(1)


def dl_signal_handler(signum, frame):
    src_ret = 0
    bin_ret = 0

    logger.info("Received signal of keyboard interrupt")
    if binary_dl:
        bin_ret = binary_dl.reports()
    if source_dl:
        src_ret = source_dl.reports()
    sys.exit(src_ret + bin_ret)


def dl_register_signal_handler():
    signal.signal(signal.SIGINT, dl_signal_handler)
    signal.signal(signal.SIGHUP, dl_signal_handler)
    signal.signal(signal.SIGTERM, dl_signal_handler)


if __name__ == "__main__":
    binary_dl = None
    source_dl = None
    binary_ret = 0
    source_ret = 0
    distro = STX_DEFAULT_DISTRO
    distro_codename = STX_DEFAULT_DISTRO_CODENAME
    layers = None
    build_types = None

    parser = argparse.ArgumentParser(description="downloader helper")
    parser.add_argument('-b', '--download_binary', help="download binary debs",
                        action='store_true')
    parser.add_argument('-s', '--download_source', help="download stx source",
                        action='store_true')
    parser.add_argument('-c', '--clean_mirror', help="clean the whole mirror and download again, be careful to use",
                        action='store_true')
    parser.add_argument('-d', '--distro', type=str,
                        help="name of the distro to build\n   %s" % ALL_DISTROS,
                        default=STX_DEFAULT_DISTRO, required=False)
    parser.add_argument('-C', '--distro_codename', type=str,
                        help="name of the distro codename to build\n   %s" % discovery.STX_DISTRO_DICT,
                        default=STX_DEFAULT_DISTRO_CODENAME, required=False)
    parser.add_argument('-B', '--build-types', type=str,
                        help="comma separated list of all build-types to build\n   %s" % ALL_BUILD_TYPES,
                        default=','.join(ALL_BUILD_TYPES), required=False)
    parser.add_argument('-l', '--layers', type=str,
                        help="comma separated list of all layers to build\n   %s" % ALL_LAYERS,
                        default=None, required=False)

    args = parser.parse_args()
    clean_mirror = args.clean_mirror

    if args.distro:
        if args.distro not in ALL_DISTROS:
            logger.error('Distro "{}" is not supported. It should be one of: [{}]'.format(
                args.distro,
                ','.join(ALL_DISTROS)))
            logger.error("Please consult: downloader --help")
            sys.exit(1)
        distro = args.distro

    if args.distro_codename:
        if args.distro_codename not in discovery.STX_DISTRO_DICT[distro]:
            logger.error('Distro codname "{}" is not supported for {}. It should be one of: [{}]'.format(
                args.distro_codename,
                distro, 
                ','.join(discovery.STX_DISTRO_DICT[distro])))
            logger.error("Please consult: downloader --help")
            sys.exit(1)
        distro_codename = args.distro_codename

    if args.build_types:
        build_types = args.build_types.strip().split(',')
        logger.debug("The required types to download: %s", ','.join(build_types))
        for build_type in build_types:
            if build_type not in ALL_BUILD_TYPES:
                logger.error(' '.join(['Build_type', build_type, 'not in', ','.join(ALL_BUILD_TYPES)]))
                logger.error("Please consult: downloader --help")
                sys.exit(1)

    # Verify that this container build suite has the correct tooling for this distro/codename
    os_release_dict = {}
    with open("/etc/os-release") as myfile:
        for line in myfile:
            key, val = line.partition("=")[::2]
            os_release_dict[key.strip()] = val.strip()

    if distro != os_release_dict['ID'] or distro_codename != os_release_dict['VERSION_CODENAME']:
        logger.error('Invalid tooling. You are attempting to download {}/{} with {}/{} containers'.format(
            distro, distro_codename, os_release_dict['ID'], os_release_dict['VERSION_CODENAME']))
        sys.exit(1)
    else:
        logger.info('Tooling validated. You are downloading {}/{} with {}/{} containers'.format(
            distro, distro_codename, os_release_dict['ID'], os_release_dict['VERSION_CODENAME']))

    # Reset these based on potentially new values
    ALL_LAYERS = discovery.get_all_layers(distro=distro, codename=distro_codename)
    ALL_BUILD_TYPES = discovery.get_all_build_types(distro=distro)

    if args.layers:
        layers = args.layers.strip().split(',')
        for layer in layers:
            logger.info("layer=%s" % layer)
            if layer not in ALL_LAYERS:
                logger.error(' '.join(['Layer', layer, 'not in', ','.join(ALL_LAYERS)]))
                logger.error("Please consult: downloader --help")
                sys.exit(1)

    if not args.download_binary and not args.download_source:
        # Default to binary and source when option is not provided
        args.download_binary = True
        args.download_source = True

    logger.info('Setting up apt cache')
    
    create_apt_chroot()
    apt_update_inside_chroot()
    logger.info('Apt cache is ready')

    dl_list_dir = '%s/required_downloads' % os.environ['MY_WORKSPACE']
    if os.path.isdir(dl_list_dir):
        shutil.rmtree(dl_list_dir)
    os.makedirs(dl_list_dir, exist_ok=True)
    if args.download_binary:
        all_binary_lists = get_all_binary_list(distro=distro, codename=distro_codename, layers=layers, build_types=build_types)
        dl_list_file_bin = '%s/binaries.txt' % dl_list_dir
        binary_dl = DebDownloader(DEFAULT_ARCH, stx_bin_mirror, dl_list_file_bin, clean_mirror, all_binary_lists)
        if not binary_dl.create_binary_repo():
            sys.exit(1)

    if args.download_source:
        dl_list_file_src = '%s/sources.txt' % dl_list_dir
        source_dl = SrcDownloader(DEFAULT_ARCH, stx_src_mirror, dl_list_file_src, clean_mirror,
                                  distro=distro, codename=distro_codename)

    dl_register_signal_handler()
    if binary_dl:
        binary_dl.start()
    if source_dl:
        source_dl.start(layers=layers, build_types=build_types)

    if binary_dl:
        binary_ret = binary_dl.reports()
    if source_dl:
        logger.info('Show the download result for source packages:')
        source_ret = source_dl.reports()

    # sort required_download lists
    for dl_list_file in glob.glob('%s/*.txt' % dl_list_dir):
        if os.path.isfile(dl_list_file):
            cmd = 'file="%s" && sort -u "$file" >"$file".tmp && mv -f "$file".tmp "$file"' % dl_list_file
            utils.run_shell_cmd(cmd, logger)

    logger.info('Required download file names are in %s/', dl_list_dir)

    logger.info("Verifying downloader return status")
    if binary_ret != 0:
        logger.error("Binary downloader failed")
        binary_dl.list_failed_pkgs()
    else:
        logger.info("All binaries were downloaded")

    if source_ret != 0:
        logger.error("Source downloader failed")
        source_dl.list_failed_pkgs()
    else:
        logger.info("All sources were downloaded")

    logger.info("Downloader done")
    sys.exit(binary_ret + source_ret)
